<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Dates and times with Lubridate</title>

<script src="1-lubridate_files/header-attrs-2.21/header-attrs.js"></script>
<script src="1-lubridate_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="1-lubridate_files/bootstrap-3.3.5/css/lumen.min.css" rel="stylesheet" />
<script src="1-lubridate_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="1-lubridate_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="1-lubridate_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="1-lubridate_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="1-lubridate_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="1-lubridate_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="1-lubridate_files/navigation-1.1/tabsets.js"></script>
<link href="1-lubridate_files/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="1-lubridate_files/pagedtable-1.1/js/pagedtable.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #204a87; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #204a87; font-weight: bold; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #ce5c00; font-weight: bold; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<link rel="stylesheet" href="custom.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div id="header">



<h1 class="title toc-ignore">Dates and times with Lubridate</h1>
<h3 class="subtitle">I2DS Tools for Data Science Workshop</h3>

</div>


<hr />
<p>In this workshop, we‚Äôll introduce you to the Lubridate package, a
useful tool to work with dates and times in R.</p>
<p>Objectives of the session:</p>
<ul>
<li><p>Understand the core components of lubridate.</p></li>
<li><p>To learn how to handle date and time data effectively.</p></li>
<li><p>To apply this knowledge to real-world data analysis and
visualization tasks.</p></li>
</ul>
<div id="what-is-lubridate" class="section level1">
<h1>What is lubridate? ‚è≥</h1>
<p>Lubridate simplifies date and time data manipulation in R.</p>
<p>It provides an intuitive and consistent interface for
<strong>creating, parsing, and performing operations on date-time
objects.</strong></p>
<p>This package offers a set of standardized functions for analyze
dates, performing arithmetic with date-time data, and extracting
specific components of date and time formats (e.g., year, month,
day).</p>
</div>
<div id="scraping-static-sites-with-rvest" class="section level1">
<h1>Scraping static sites with <code>rvest</code> üï•</h1>
<p>Who doesn‚Äôt love Wikipedia? Let‚Äôs use this as our first, straight
forward test case.</p>
<p><strong>Step 1.</strong> Load the packages <code>rvest</code> and
<code>stringr</code>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># install.packages(&quot;devtools&quot;)</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co"># devtools::install_github(&quot;hadley/emo&quot;)</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="fu">library</span>(lubridate)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="fu">library</span>(emo)</span></code></pre></div>
<p><strong>Step 2.</strong> Parse the page source.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>my_duration <span class="ot">&lt;-</span> <span class="fu">dhours</span>(<span class="dv">2</span>) <span class="sc">+</span> <span class="fu">dminutes</span>(<span class="dv">30</span>)</span></code></pre></div>
<p><strong>Step 3.</strong> Extract information.</p>
<p>There are two options:</p>
<p><strong>Option 1.</strong> We saw how to do this before the Midterm
brea. On your page of interest, go to a table that you‚Äôd like to scrape.
Our favorite bowser for webscraping is Google Chrome but others work as
well. On Chrome, you go in View &gt; Developer &gt; inspect elements. If
you hover over the html code on the right, you should see boxes of
different colors framing different elements of the page. Once the part
of the page you would like to scrape is selected, right click on the
html code and Copy &gt; Copy Xpath. That‚Äôs it.</p>
<p><strong>Option 2.</strong> Something we did not show you, but that
you might look at in your own time is the Chrome Extension
<code>SelectorGadget</code>. You download the <a
href="https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=de">Chrome
Extension</a> <code>SelectorGadget</code> and activate it while browsing
the page you‚Äôd like to scrape from. You will see a selection box moving
with your cursor. You select an element by clickin on it. It turns green
- and so does all other content that would be selected with the current
XPath.</p>
<p>You can now de-select everything that is irrelevant to you by
clicking it again (it then turns red). Final step, then just click the
XPath button at the bottom of the browser window. Make sure to use
single quotation marks with this XPath!</p>
<p>Let‚Äôs repeat step 2 and 3 with a more data-sciency example. üéì</p>
<p><strong>Step 2.</strong> Parse the page source.</p>
<p><strong>Step 3.</strong> Extract information. When going through
different levels of html, you can also use tidyverse logic.</p>
<pre><code>body_nodes &lt;- nypl100 |&gt; 
 html_elements(&quot;body&quot;) |&gt; 
 html_children()

body_nodes |&gt; 
 html_children()</code></pre>
<p>play with that yourself if you like‚Ä¶</p>
<p>Now let‚Äôs have a look at the different ways to extract
information:</p>
<p><strong>Step 4.</strong> Usually, step 4 is to clean extracted data.
In this case, it actually is pretty clean already, thanks to
<code>html_text2()</code>. However, in many cases, we need to clean the
data we scraped with regular expressions.</p>
<p><strong>Step 5.</strong> Put everything into a data frame. üìñ</p>
<div id="scraping-html-tables" class="section level2">
<h2>Scraping HTML tables üöÄ</h2>
<p>Oftentimes, we would like to scrape tabular data from the web. This
is even easier in <code>rvest</code>!</p>
</div>
</div>
<div id="scraping-multiple-pages" class="section level1">
<h1>Scraping multiple pages ü§ñ</h1>
<p>Whenever you want to really understand what‚Äôs going on within the
functions of a new R package, it is very likely that there is a relevant
article published in the <a
href="https://www.jstatsoft.org/index">Journal of Statistical
Software</a>. Let‚Äôs say you are interested in how the journal was doing
over the past years.</p>
<p><strong>Step 1.</strong> Inspect the source. Basically, follow steps
to extract the Xpath information.</p>
<p><strong>Step 2</strong> Develop a scraping strategy. We need a set of
URLs leading to all sources. Inspect the URLs of different sources and
find the pattern. Then, construct the list of URLs from scratch.</p>
<p><strong>Step 3</strong> Think about where you want your scraped
material to be stored and create a directory.</p>
<p><strong>Step 4</strong> Download the pages. Note that we did not do
this step last time, when we were only scraping one page.</p>
<p>While R is downloading the pages for you, you can watch it directly
in the directory you defined‚Ä¶</p>
<p>Check whether it worked.</p>
<p>Yay! Apparently, we scraped the html pages of 855 articles.</p>
<div id="gitignoring-files" class="section level2">
<h2>(Git)ignoring files üôÖ</h2>
<p>In case you scraping project is is linked to GitHub (as it will be in
your assignment!), it can be useful to <strong>.gitignore</strong> the
folder of downloaded files. This means that the folder can be stored in
your local directory of the project but will not be synced with the
remote (main) repository. Here is information on how to do this using <a
href="https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html">RStudio</a>.
In Github Desktop it is very simple, you do your scraping work, the
folder is created in your local repository and before your commit and
push these changes, you go on <code>Repository</code> &gt;
<code>Repository Settings</code> &gt; <code>Ignored Files</code> and
edit the .gitignore file (add the name of the new folder / files you
don‚Äôt want to sync). More generally, it makes sense to exclude .Rproj
files, .RData files (and other binary or large data files), draft
folders and sensitive information from version control. Remember, git is
built to track changes in code, not in large data files.</p>
<p><strong>Step 5</strong> Import files and parse out information. A
loop is helpful here!</p>
<p><strong>Step 6</strong> Clean data‚Ä¶</p>
<p>You see, scraping data from multiple pages is no problem in R. Most
of the brain work often goes into developing a scraping strategy and
tidying the data, not into the actual downloading/scraping part.</p>
<p>Scraping is also possible in much more complex scenarios! Watch out
for workshop presentations on</p>
<ul>
<li>Text analytics with <code>quanteda</code></li>
<li>Regular expressions with <code>stringr</code></li>
<li>Data cleaning with <code>janitor</code></li>
</ul>
<p>and many more ü§©</p>
</div>
</div>
<div id="good-scraping-practice" class="section level1">
<h1>Good scraping practice</h1>
<p>There is a set of general rules to the game:</p>
<ol style="list-style-type: decimal">
<li>You take all the <strong>responsibility</strong> for your web
scraping work.</li>
<li>Think about the nature of the data. Does it entail <strong>sensitive
information</strong>? Do not collect personal data without explicit
permission.</li>
<li>Take all <strong>copyrights</strong> of a country‚Äôs jurisdiction
into account. If you publish data, do not commit copyright fraud.</li>
<li>If possible, <strong>stay identifiable</strong>. Stay polite. Stay
friendly. Obey the scraping etiquette.</li>
<li>If in doubt, <strong>ask the author/creator/provider</strong> of
data for permission‚Äîif your interest is entirely scientific, chances
aren‚Äôt bad that you get data.</li>
</ol>
<div id="how-do-i-know-the-scraping-etiquette-of-a-site"
class="section level2">
<h2>How do I know the scraping etiquette of a site? ü§ù</h2>
<p>Robot exclusion standards (<code>robot.txt</code>) are informal
protocols to prohibit web robots from crawling content. They list
documents that are allowed to crawl and which not. It is not a technical
barrier but an ask for compliance. They are located in the root
directory of a website (e.g
<code>https://de.wikipedia.org/robots.txt</code>).</p>
<p>For example, let‚Äôs have a look at wikipedia‚Äôs <a
href="https://de.wikipedia.org/robots.txt">robot.txt</a> file, which is
very human readable.</p>
<p>General rules are listed under <code>User-agent: *</code> which is
most interesting for R-based crawlers. A universal ban for a directory
looks like this <code>Disallows: /</code>, sometimes Crawl-delays are
suggested (in seconds) <code>Crawl-delay: 2</code>.</p>
</div>
<div id="what-is-polite-scraping" class="section level2">
<h2>What is ‚Äúpolite‚Äù scraping? üêå</h2>
<p>First thing would be not to scrape at a speed that causes trouble for
their server. Therefore, whenever you loop over a list of URLs, add a
<code>Sys.sleep(runif(1, 1, 2))</code> at the end of the loop.</p>
<p>And generally, it is better practice to store data on your local
drive first (<code>download.file()</code>), then parse
(<code>read_html()</code>).</p>
<p><strong>A footnote on sustainability.</strong> In the digital
context, we often forget that or actions do have physical consequences.
For example, training AI, using blockchain and just streaming videos do
cause considerable amounts of <span class="math inline">\(CO^2\)</span>
emissions. So does bombarding a server with requests - certainly to a
much lesser extent than the examples before - but please consider
whether you have to re-run a large scraping project 100 times in order
to debug things.</p>
<p>Furthermore, downloading massive amounts of data may arouse attention
from server administrators. Assuming that you‚Äôve got nothing to hide,
you should stay identifiable beyond your IP address.</p>
</div>
<div id="how-can-i-stay-identifyable" class="section level2">
<h2>How can I stay identifyable? üë§</h2>
<p>Option 1: Get in touch with website administrators / data owners.</p>
<p>Option 2: Use HTTP header fields From and User-Agent to provide
information about yourself.</p>
<pre><code> </code></pre>
<p>rvest‚Äôs <code>session()</code> creates a session object that responds
to HTTP and HTML methods. Here, we provide our email address and the
current R version as User-Agent information. This will pop up in the
server logs: The webpage administrator has the chance to easily get in
touch with you.</p>
</div>
</div>
<div id="on-an-api-far-far-away" class="section level1">
<h1>On an API far, far away‚Ä¶ ‚≠ê</h1>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="fu">library</span>(httr)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="fu">library</span>(jsonlite)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="fu">library</span>(xml2)</span></code></pre></div>
<p>To get data from an API, we suggest to follow a workflow like
this:</p>
<ol style="list-style-type: decimal">
<li>Read the APIs documentation!</li>
<li>Get the baseurl</li>
<li>Find out the parameters referring to the resources of interest to
you</li>
<li>Create a query url from the base url and the query parameters</li>
<li>Run the <code>GET</code> function on the query url</li>
<li>Depending on the encoding (usually, it‚Äôs json), you will need
to:</li>
</ol>
<ul>
<li>Parse the result with the <code>content</code> function</li>
<li>Either use <code>jsonlite</code> or <code>xml2</code> to parse the
json or xml files</li>
</ul>
<p>Let‚Äôs have a look at an example, the <a
href="https://swapi.dev/">Star Wars API</a>:</p>
</div>

&nbsp;
<hr />
<p style="text-align: center;">A work by Monserrat L√≥pez and . Template of Lisa Oswald & Tom Arend.</a></p>
<p style="text-align: center;"><span style="color: #808080;"><em>I2DS Tools for Data Science Workshop by Simon Munzert</em></span></p>
<p style="text-align: center;"><span style="color: #808080;"><em><a href="https://www.hertie-school.org/en/">Hertie School, Berlin</em></span></p>

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" >

<!-- Add font awesome icons -->
<p style="text-align: center;">
    <a href="https://github.com/intro-to-data-science-23"  <i class="fab fa-github"></i><a>
</p>

&nbsp;


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
